<p>CSE 546: Reinforcement LearningFall 2024Assignment - 2Check Point 2 ReportInstructor: Alina VereshchakaDeadline: Oct/24/2024Team 11Raja Sekhar Mora - rmora3Sudip Kumar Sengupta - sudipkum1. Using Experience Replay in DQN and Its Influence on ResultsExperience replay is a technique where transitions (state, action, reward, next state, done) arestored in a buffer, and a random sample from this buffer is used for training the neuralnetwork.Benefits of Experience Replay:•Breaking Temporal Correlations: In standard reinforcement learning, consecutivestates are highly correlated. Training on these correlated samples leads to poorconvergence. Experience replay randomises the samples by sampling from a buffer,which breaks the temporal correlations and stabilises training.•Efficient Sample Reuse: Instead of discarding experiences immediately after theyoccur, they are stored in a buffer, allowing the agent to learn from them multiple times.This improves sample efficiency, especially in environments where data collection isexpensive.•Reduced Variance: Since experiences are drawn randomly from the buffer, thenetwork can learn from a broader range of state transitions, reducing the variance ingradient updates.How Experience Replay Size Influences Results:•Small Buffer Size: If the buffer is too small, the agent may overfit to the recentexperiences. Small buffers reduce the diversity of training samples, which can result inslower or unstable learning.•Large Buffer Size: Larger buffers increase sample diversity and help preventoverfitting to recent experiences. However, if the buffer is too large, it may end upretaining old and irrelevant experiences, which can reduce the learning efficiency.•Balance: In practice, experience replay buffer sizes are typically set between 10,000to 1,000,000 transitions, depending on the problem. The balance between buffer size andcomputational resources is crucial for good performance.2. Introducing the Target NetworkIn DQN, the target network is a separate neural network that is used to calculate the targetQ-values in the Bellman equation. It is updated less frequently than the main Q-network.Benefits of Using a Target Network:•Stabilising Learning: If the Q-network updates its weights after each gradient stepand is also used to compute target Q-values, the target itself keeps changing, leading toinstability (since the learning target is moving too fast). The target network decouples thetarget Q-values from the Q-network, stabilising the learning process.•Fixed Target for Gradients: By keeping the target Q-values fixed for several steps,the gradients computed from the loss function are more stable. This prevents oscillationsand divergence that could arise if the Q-values changed too drastically after every update.•Periodic Updates: The target network is updated with the Q-network’s weightsperiodically (e.g., every 10 episodes). This slow update provides a “smoother” change inthe Q-values, making learning more robust.Representing the Q-Function as Q(s, a; w)In DQN, the Q-function is represented by a neural network Q(s, a; w) , where w representsthe weights of the neural network. This representation allows the Q-function to approximatethe action-value function in high-dimensional state spaces.Benefits of Representing Q as a Function Approximator:•Generalisation to Unseen States: Using a neural network to represent the Q-functionallows the agent to generalise across similar states, rather than requiring a separateQ-value for every possible state-action pair. This is crucial in environments withcontinuous or large state spaces.•Handling Large State Spaces: In traditional Q-learning, the Q-values for allstate-action pairs are stored in a table, which becomes infeasible in large or continuousstate spaces. Neural networks approximate Q-values for any state-action pair efficiently,overcoming this limitation.•Nonlinear Function Approximation: By using a deep neural network withnon-linear activations (e.g., ReLU), the Q-function can approximate complex, non-linearrelationships between states and actions, capturing more intricate patterns in theenvironment’s dynamics.•Scalability: Neural networks with millions of parameters can scale to complexenvironments, which would be impossible with a tabular approach. Representing theQ-function as Q(s, a; w) is what allows DQN to succeed in environments like Atarigames, where the state space is huge.1. CartPole-v1•Agent: A cart that moves along a track.•Goal: Balance a pole upright on the moving cart.•State: The state is a 4-dimensional vector, representing:•Cart position (x-coordinate)•Cart velocity•Pole angle (from vertical)•Pole’s angular velocity•Actions:•0: Push the cart to the left•1: Push the cart to the right•Rewards:•+1 for every time step the pole remains upright.•Episode End:•The pole falls beyond a certain angle (±12°) or the cart moves out of bounds (|x| &gt; 2.4).•Maximum steps per episode: 500.2. LunarLander-v2•Agent: A lunar lander spacecraft.•Goal: Safely land the spacecraft between designated flags on a lunar surface.•State: The state is an 8-dimensional vector, representing:•Lander’s position (x, y)•Lander’s velocity (x, y)•Lander’s angle•Angular velocity•Boolean flags indicating whether the left or right leg has landed.•Actions:•0: Do nothing•1: Fire left engine•2: Fire main engine•3: Fire right engine•Rewards:•+100 to +140 for a successful landing (depending on proximity to the landing pad).•-100 for crashing or flying out of bounds.•-0.3 for using the main engine (fuel consumption).•Small rewards for each leg contacting the ground.•Episode End:•The lander crashes or flies out of bounds, or successfully lands.3. Warehouse Grid-World Environment (Custom)•Agent: A robot navigating in a grid-based warehouse environment.•Goal: The robot must pick up and deliver items to specified locations while avoiding obstacles.•State:•The state is a 2D grid (rows, columns), representing the robot’s position, the positions ofobstacles, items to be picked up, and delivery points.•Additional features may include robot orientation and task-specific details (e.g., itemheld, location to deliver).•Actions:•Move Up•Move Down•Move Left•Move Right•Pick up item•Drop off item•Rewards:•Positive reward for successfully delivering items.•Negative rewards for hitting obstacles or moving inefficiently.•Penalties for running into walls or taking unnecessary steps.•Episode End:•The agent completes the delivery or fails (e.g., exceeds time steps or hits an obstacle).After applying DQN Network on the above three environments we have the followingresultsRobot Warehouse Environment -Cartpole - v1 Environment -Lunar Lander Environment -Evaluation of the respective models on each environment -</p>
